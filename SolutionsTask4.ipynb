{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_dream_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNntDX8oAsDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPT0_Ob9BDUG",
        "colab_type": "code",
        "outputId": "8f2558e9-1e4c-4c41-b806-f035933033e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "#We will load the MNIST dataset here.\n",
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root = './data', train = True, download = True, transform = transform )\n",
        "\n",
        "test_data = torchvision.datasets.MNIST(root = './data',train = False ,download = True,  transform = transform)\n",
        "\n",
        " \n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "plt.imshow(np.asarray(train_data[1][0].reshape(28,28)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f535a62aba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRmiRtSDB_QV",
        "colab_type": "code",
        "outputId": "a09f797e-1670-463b-a228-02f0a9865f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_loader = torch.utils.data.DataLoader (train_data, batch_size = BATCH_SIZE, )\n",
        "test_loader = torch.utils.data.DataLoader (test_data, batch_size = 1, )\n",
        "\n",
        "# train_loader.to('cuda')\n",
        "# test_loader.to('cuda')\n",
        "\n",
        "\n",
        "print(train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f530cf3c780>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMeoiIqrd-v7",
        "colab_type": "text"
      },
      "source": [
        "We will define our network in the following cell. There are several ways of doing this in PyTorch, but in this example we will do it by subclassing nn.module.\n",
        "\n",
        "Our network will have 4 convolutional layers followed by 2 fully-connected layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHW6Cku-CJSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size = 3) #Only black and while input channel.\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n",
        "    self.conv3 = nn.Conv2d(64, 64, kernel_size = 5)\n",
        "    self.conv4 = nn.Conv2d(64, 64, kernel_size = 5)\n",
        "\n",
        "    self.dense1 = nn.Linear(64*3*3, 64)\n",
        "    self.dense2 = nn.Linear(64, 128)\n",
        "    self.dense3 = nn.Linear(128, 64)\n",
        "    self.dense4 = nn.Linear(64, 16)\n",
        "    self.dense5 = nn.Linear(16, 1)\n",
        "\n",
        "  def forward (self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.relu(self.conv2(x))\n",
        "\n",
        "    x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv4(x), 2))\n",
        "\n",
        "    x = x.view(-1, 3*3*64)\n",
        "    x = F.relu(self.dense1(x))\n",
        "    x = F.relu(self.dense2(x))\n",
        "    x = F.relu(self.dense3(x))\n",
        "    x = F.relu(self.dense4(x))\n",
        "    x = self.dense5(x)\n",
        "\n",
        "\n",
        "    return x    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T2rbFGKlS32",
        "colab_type": "code",
        "outputId": "8d88d6f9-e8d8-42e0-faff-e36c234c0209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "#Setting few things up\n",
        "\n",
        "net = MyConvNet()\n",
        "net.to('cuda')\n",
        "print(net)\n",
        "tld = iter(train_loader)\n",
        "im = next(tld)[0].to(device)\n",
        "print (net.forward(im))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyConvNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (dense1): Linear(in_features=576, out_features=64, bias=True)\n",
            "  (dense2): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (dense3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (dense4): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (dense5): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "tensor([[0.2167],\n",
            "        [0.2164],\n",
            "        [0.2160],\n",
            "        [0.2166],\n",
            "        [0.2162],\n",
            "        [0.2163],\n",
            "        [0.2164],\n",
            "        [0.2166],\n",
            "        [0.2162],\n",
            "        [0.2164],\n",
            "        [0.2165],\n",
            "        [0.2163],\n",
            "        [0.2164],\n",
            "        [0.2167],\n",
            "        [0.2164],\n",
            "        [0.2162],\n",
            "        [0.2166],\n",
            "        [0.2168],\n",
            "        [0.2163],\n",
            "        [0.2165],\n",
            "        [0.2158],\n",
            "        [0.2164],\n",
            "        [0.2164],\n",
            "        [0.2165],\n",
            "        [0.2162],\n",
            "        [0.2166],\n",
            "        [0.2160],\n",
            "        [0.2166],\n",
            "        [0.2168],\n",
            "        [0.2163],\n",
            "        [0.2162],\n",
            "        [0.2169]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYgE1F24lXn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    correct = 0 \n",
        "    for test_imgs, test_labels in test_loader:\n",
        "        test_imgs = Variable(test_imgs).to(device)\n",
        "        output = model(test_imgs)\n",
        "        predicted = torch.max(output,axis = 1).indices\n",
        "        correct += (predicted == test_labels).sum()\n",
        "        # print(predicted, test_labels)\n",
        "        # print(correct)\n",
        "    print(\"Test accuracy:{:.3f}% \".format( float(correct * 100) / (len(test_loader))))\n",
        "\n",
        "\n",
        "def train(model, train_loader, EPOCHS = 25, lossF = None):\n",
        "  if lossF == None:\n",
        "    lossF = nn.MSELoss()\n",
        "  optim = torch.optim.Adam (model.parameters(), lr = 4e-5, weight_decay=1e-7)\n",
        "  model.train()\n",
        "  for epoch in range(EPOCHS):\n",
        "    correct = 0\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "      var_X_batch = Variable(X_batch).to(device)\n",
        "      var_y_batch = Variable(y_batch).to(device)\n",
        "      # print(var_X_batch.size())\n",
        "      output = model(var_X_batch)\n",
        "\n",
        "      # print(output)\n",
        "      # print(output.size(), var_y_batch.size())\n",
        "      loss = lossF (output.float().view(32), var_y_batch.float().view(32))\n",
        "      # loss = torch.exp(loss)\n",
        "      # print(loss)\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      optim.zero_grad()\n",
        "\n",
        "      predicted = output\n",
        "      # print(predicted)\n",
        "      # print (torch.abs(predicted.view(-1) - var_y_batch.view(-1)) < 0.45)\n",
        "      correct += (torch.abs(predicted.view(-1) - var_y_batch.view(-1)) < 0.49999).sum()\n",
        "\n",
        "      \n",
        "      if (batch_idx % 200) == 0:\n",
        "          print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
        "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnWa2Cx46lt",
        "colab_type": "code",
        "outputId": "855a0775-5413-44ff-8b1b-8d63da6a1864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train (net, train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 [0/60000 (0%)]\tLoss: 20.932194\t Accuracy:6.250%\n",
            "Epoch : 0 [6400/60000 (11%)]\tLoss: 5.830895\t Accuracy:8.815%\n",
            "Epoch : 0 [12800/60000 (21%)]\tLoss: 4.525723\t Accuracy:8.339%\n",
            "Epoch : 0 [19200/60000 (32%)]\tLoss: 7.278857\t Accuracy:8.143%\n",
            "Epoch : 0 [25600/60000 (43%)]\tLoss: 3.662749\t Accuracy:8.294%\n",
            "Epoch : 0 [32000/60000 (53%)]\tLoss: 5.199687\t Accuracy:9.178%\n",
            "Epoch : 0 [38400/60000 (64%)]\tLoss: 2.524659\t Accuracy:10.249%\n",
            "Epoch : 0 [44800/60000 (75%)]\tLoss: 2.015491\t Accuracy:11.572%\n",
            "Epoch : 0 [51200/60000 (85%)]\tLoss: 3.170655\t Accuracy:12.886%\n",
            "Epoch : 0 [57600/60000 (96%)]\tLoss: 1.567123\t Accuracy:14.403%\n",
            "Epoch : 1 [0/60000 (0%)]\tLoss: 2.476189\t Accuracy:18.750%\n",
            "Epoch : 1 [6400/60000 (11%)]\tLoss: 1.656231\t Accuracy:30.846%\n",
            "Epoch : 1 [12800/60000 (21%)]\tLoss: 1.333561\t Accuracy:32.357%\n",
            "Epoch : 1 [19200/60000 (32%)]\tLoss: 1.616020\t Accuracy:32.836%\n",
            "Epoch : 1 [25600/60000 (43%)]\tLoss: 1.032303\t Accuracy:34.024%\n",
            "Epoch : 1 [32000/60000 (53%)]\tLoss: 3.885685\t Accuracy:34.806%\n",
            "Epoch : 1 [38400/60000 (64%)]\tLoss: 1.588501\t Accuracy:35.645%\n",
            "Epoch : 1 [44800/60000 (75%)]\tLoss: 1.074043\t Accuracy:36.773%\n",
            "Epoch : 1 [51200/60000 (85%)]\tLoss: 1.512940\t Accuracy:37.408%\n",
            "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.969237\t Accuracy:38.555%\n",
            "Epoch : 2 [0/60000 (0%)]\tLoss: 0.826873\t Accuracy:62.500%\n",
            "Epoch : 2 [6400/60000 (11%)]\tLoss: 1.069199\t Accuracy:49.518%\n",
            "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.555354\t Accuracy:49.330%\n",
            "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.747972\t Accuracy:49.303%\n",
            "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.883080\t Accuracy:49.821%\n",
            "Epoch : 2 [32000/60000 (53%)]\tLoss: 2.879134\t Accuracy:49.563%\n",
            "Epoch : 2 [38400/60000 (64%)]\tLoss: 1.042392\t Accuracy:49.792%\n",
            "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.914922\t Accuracy:50.234%\n",
            "Epoch : 2 [51200/60000 (85%)]\tLoss: 1.102172\t Accuracy:50.076%\n",
            "Epoch : 2 [57600/60000 (96%)]\tLoss: 1.002088\t Accuracy:50.578%\n",
            "Epoch : 3 [0/60000 (0%)]\tLoss: 0.479522\t Accuracy:62.500%\n",
            "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.742464\t Accuracy:56.950%\n",
            "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.413614\t Accuracy:56.375%\n",
            "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.573251\t Accuracy:56.240%\n",
            "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.864244\t Accuracy:56.613%\n",
            "Epoch : 3 [32000/60000 (53%)]\tLoss: 2.212916\t Accuracy:56.262%\n",
            "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.806759\t Accuracy:56.625%\n",
            "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.841860\t Accuracy:56.937%\n",
            "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.921228\t Accuracy:56.703%\n",
            "Epoch : 3 [57600/60000 (96%)]\tLoss: 1.068706\t Accuracy:57.097%\n",
            "Epoch : 4 [0/60000 (0%)]\tLoss: 0.440291\t Accuracy:65.625%\n",
            "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.544272\t Accuracy:62.904%\n",
            "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.425190\t Accuracy:62.352%\n",
            "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.482158\t Accuracy:61.751%\n",
            "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.943095\t Accuracy:61.923%\n",
            "Epoch : 4 [32000/60000 (53%)]\tLoss: 1.833685\t Accuracy:61.535%\n",
            "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.665473\t Accuracy:61.993%\n",
            "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.717666\t Accuracy:62.284%\n",
            "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.697756\t Accuracy:62.092%\n",
            "Epoch : 4 [57600/60000 (96%)]\tLoss: 1.027201\t Accuracy:62.427%\n",
            "Epoch : 5 [0/60000 (0%)]\tLoss: 0.402075\t Accuracy:71.875%\n",
            "Epoch : 5 [6400/60000 (11%)]\tLoss: 0.434537\t Accuracy:67.926%\n",
            "Epoch : 5 [12800/60000 (21%)]\tLoss: 0.455871\t Accuracy:66.997%\n",
            "Epoch : 5 [19200/60000 (32%)]\tLoss: 0.442952\t Accuracy:66.259%\n",
            "Epoch : 5 [25600/60000 (43%)]\tLoss: 0.908462\t Accuracy:66.518%\n",
            "Epoch : 5 [32000/60000 (53%)]\tLoss: 1.570998\t Accuracy:66.084%\n",
            "Epoch : 5 [38400/60000 (64%)]\tLoss: 0.503331\t Accuracy:66.502%\n",
            "Epoch : 5 [44800/60000 (75%)]\tLoss: 0.585012\t Accuracy:66.818%\n",
            "Epoch : 5 [51200/60000 (85%)]\tLoss: 0.516024\t Accuracy:66.583%\n",
            "Epoch : 5 [57600/60000 (96%)]\tLoss: 0.995840\t Accuracy:66.827%\n",
            "Epoch : 6 [0/60000 (0%)]\tLoss: 0.377241\t Accuracy:78.125%\n",
            "Epoch : 6 [6400/60000 (11%)]\tLoss: 0.360627\t Accuracy:72.326%\n",
            "Epoch : 6 [12800/60000 (21%)]\tLoss: 0.403123\t Accuracy:70.846%\n",
            "Epoch : 6 [19200/60000 (32%)]\tLoss: 0.402259\t Accuracy:70.216%\n",
            "Epoch : 6 [25600/60000 (43%)]\tLoss: 0.866648\t Accuracy:70.471%\n",
            "Epoch : 6 [32000/60000 (53%)]\tLoss: 1.323181\t Accuracy:70.170%\n",
            "Epoch : 6 [38400/60000 (64%)]\tLoss: 0.431413\t Accuracy:70.441%\n",
            "Epoch : 6 [44800/60000 (75%)]\tLoss: 0.540052\t Accuracy:70.753%\n",
            "Epoch : 6 [51200/60000 (85%)]\tLoss: 0.449322\t Accuracy:70.470%\n",
            "Epoch : 6 [57600/60000 (96%)]\tLoss: 0.882419\t Accuracy:70.693%\n",
            "Epoch : 7 [0/60000 (0%)]\tLoss: 0.342188\t Accuracy:75.000%\n",
            "Epoch : 7 [6400/60000 (11%)]\tLoss: 0.336590\t Accuracy:75.389%\n",
            "Epoch : 7 [12800/60000 (21%)]\tLoss: 0.399253\t Accuracy:73.815%\n",
            "Epoch : 7 [19200/60000 (32%)]\tLoss: 0.348291\t Accuracy:73.175%\n",
            "Epoch : 7 [25600/60000 (43%)]\tLoss: 0.787871\t Accuracy:73.315%\n",
            "Epoch : 7 [32000/60000 (53%)]\tLoss: 1.206412\t Accuracy:73.146%\n",
            "Epoch : 7 [38400/60000 (64%)]\tLoss: 0.376899\t Accuracy:73.379%\n",
            "Epoch : 7 [44800/60000 (75%)]\tLoss: 0.491095\t Accuracy:73.691%\n",
            "Epoch : 7 [51200/60000 (85%)]\tLoss: 0.398159\t Accuracy:73.472%\n",
            "Epoch : 7 [57600/60000 (96%)]\tLoss: 0.784919\t Accuracy:73.756%\n",
            "Epoch : 8 [0/60000 (0%)]\tLoss: 0.275757\t Accuracy:78.125%\n",
            "Epoch : 8 [6400/60000 (11%)]\tLoss: 0.237033\t Accuracy:77.876%\n",
            "Epoch : 8 [12800/60000 (21%)]\tLoss: 0.364630\t Accuracy:76.590%\n",
            "Epoch : 8 [19200/60000 (32%)]\tLoss: 0.320218\t Accuracy:75.957%\n",
            "Epoch : 8 [25600/60000 (43%)]\tLoss: 0.677264\t Accuracy:76.057%\n",
            "Epoch : 8 [32000/60000 (53%)]\tLoss: 1.069240\t Accuracy:75.965%\n",
            "Epoch : 8 [38400/60000 (64%)]\tLoss: 0.346611\t Accuracy:76.215%\n",
            "Epoch : 8 [44800/60000 (75%)]\tLoss: 0.444818\t Accuracy:76.441%\n",
            "Epoch : 8 [51200/60000 (85%)]\tLoss: 0.346316\t Accuracy:76.134%\n",
            "Epoch : 8 [57600/60000 (96%)]\tLoss: 0.740163\t Accuracy:76.399%\n",
            "Epoch : 9 [0/60000 (0%)]\tLoss: 0.234930\t Accuracy:81.250%\n",
            "Epoch : 9 [6400/60000 (11%)]\tLoss: 0.158965\t Accuracy:79.913%\n",
            "Epoch : 9 [12800/60000 (21%)]\tLoss: 0.329524\t Accuracy:78.717%\n",
            "Epoch : 9 [19200/60000 (32%)]\tLoss: 0.277798\t Accuracy:78.276%\n",
            "Epoch : 9 [25600/60000 (43%)]\tLoss: 0.578572\t Accuracy:78.476%\n",
            "Epoch : 9 [32000/60000 (53%)]\tLoss: 1.024491\t Accuracy:78.309%\n",
            "Epoch : 9 [38400/60000 (64%)]\tLoss: 0.293193\t Accuracy:78.567%\n",
            "Epoch : 9 [44800/60000 (75%)]\tLoss: 0.405565\t Accuracy:78.801%\n",
            "Epoch : 9 [51200/60000 (85%)]\tLoss: 0.291419\t Accuracy:78.455%\n",
            "Epoch : 9 [57600/60000 (96%)]\tLoss: 0.625239\t Accuracy:78.659%\n",
            "Epoch : 10 [0/60000 (0%)]\tLoss: 0.181776\t Accuracy:81.250%\n",
            "Epoch : 10 [6400/60000 (11%)]\tLoss: 0.113916\t Accuracy:81.794%\n",
            "Epoch : 10 [12800/60000 (21%)]\tLoss: 0.335362\t Accuracy:80.743%\n",
            "Epoch : 10 [19200/60000 (32%)]\tLoss: 0.264465\t Accuracy:80.335%\n",
            "Epoch : 10 [25600/60000 (43%)]\tLoss: 0.485609\t Accuracy:80.481%\n",
            "Epoch : 10 [32000/60000 (53%)]\tLoss: 0.883044\t Accuracy:80.317%\n",
            "Epoch : 10 [38400/60000 (64%)]\tLoss: 0.220422\t Accuracy:80.508%\n",
            "Epoch : 10 [44800/60000 (75%)]\tLoss: 0.363820\t Accuracy:80.712%\n",
            "Epoch : 10 [51200/60000 (85%)]\tLoss: 0.236533\t Accuracy:80.401%\n",
            "Epoch : 10 [57600/60000 (96%)]\tLoss: 0.508738\t Accuracy:80.493%\n",
            "Epoch : 11 [0/60000 (0%)]\tLoss: 0.150624\t Accuracy:84.375%\n",
            "Epoch : 11 [6400/60000 (11%)]\tLoss: 0.099424\t Accuracy:83.131%\n",
            "Epoch : 11 [12800/60000 (21%)]\tLoss: 0.333112\t Accuracy:82.255%\n",
            "Epoch : 11 [19200/60000 (32%)]\tLoss: 0.240610\t Accuracy:81.999%\n",
            "Epoch : 11 [25600/60000 (43%)]\tLoss: 0.367672\t Accuracy:82.221%\n",
            "Epoch : 11 [32000/60000 (53%)]\tLoss: 0.801771\t Accuracy:82.080%\n",
            "Epoch : 11 [38400/60000 (64%)]\tLoss: 0.203434\t Accuracy:82.239%\n",
            "Epoch : 11 [44800/60000 (75%)]\tLoss: 0.311043\t Accuracy:82.472%\n",
            "Epoch : 11 [51200/60000 (85%)]\tLoss: 0.201306\t Accuracy:82.158%\n",
            "Epoch : 11 [57600/60000 (96%)]\tLoss: 0.462386\t Accuracy:82.263%\n",
            "Epoch : 12 [0/60000 (0%)]\tLoss: 0.107637\t Accuracy:90.625%\n",
            "Epoch : 12 [6400/60000 (11%)]\tLoss: 0.100958\t Accuracy:84.422%\n",
            "Epoch : 12 [12800/60000 (21%)]\tLoss: 0.372225\t Accuracy:83.744%\n",
            "Epoch : 12 [19200/60000 (32%)]\tLoss: 0.235764\t Accuracy:83.564%\n",
            "Epoch : 12 [25600/60000 (43%)]\tLoss: 0.315908\t Accuracy:83.821%\n",
            "Epoch : 12 [32000/60000 (53%)]\tLoss: 0.715888\t Accuracy:83.669%\n",
            "Epoch : 12 [38400/60000 (64%)]\tLoss: 0.191380\t Accuracy:83.826%\n",
            "Epoch : 12 [44800/60000 (75%)]\tLoss: 0.284952\t Accuracy:84.014%\n",
            "Epoch : 12 [51200/60000 (85%)]\tLoss: 0.151183\t Accuracy:83.715%\n",
            "Epoch : 12 [57600/60000 (96%)]\tLoss: 0.365957\t Accuracy:83.809%\n",
            "Epoch : 13 [0/60000 (0%)]\tLoss: 0.091758\t Accuracy:90.625%\n",
            "Epoch : 13 [6400/60000 (11%)]\tLoss: 0.083531\t Accuracy:85.790%\n",
            "Epoch : 13 [12800/60000 (21%)]\tLoss: 0.389171\t Accuracy:85.076%\n",
            "Epoch : 13 [19200/60000 (32%)]\tLoss: 0.223907\t Accuracy:84.926%\n",
            "Epoch : 13 [25600/60000 (43%)]\tLoss: 0.247713\t Accuracy:85.155%\n",
            "Epoch : 13 [32000/60000 (53%)]\tLoss: 0.636584\t Accuracy:85.015%\n",
            "Epoch : 13 [38400/60000 (64%)]\tLoss: 0.175778\t Accuracy:85.187%\n",
            "Epoch : 13 [44800/60000 (75%)]\tLoss: 0.266250\t Accuracy:85.294%\n",
            "Epoch : 13 [51200/60000 (85%)]\tLoss: 0.123895\t Accuracy:85.007%\n",
            "Epoch : 13 [57600/60000 (96%)]\tLoss: 0.313692\t Accuracy:85.111%\n",
            "Epoch : 14 [0/60000 (0%)]\tLoss: 0.078461\t Accuracy:93.750%\n",
            "Epoch : 14 [6400/60000 (11%)]\tLoss: 0.068651\t Accuracy:86.863%\n",
            "Epoch : 14 [12800/60000 (21%)]\tLoss: 0.381814\t Accuracy:86.308%\n",
            "Epoch : 14 [19200/60000 (32%)]\tLoss: 0.233726\t Accuracy:86.184%\n",
            "Epoch : 14 [25600/60000 (43%)]\tLoss: 0.174446\t Accuracy:86.341%\n",
            "Epoch : 14 [32000/60000 (53%)]\tLoss: 0.511035\t Accuracy:86.286%\n",
            "Epoch : 14 [38400/60000 (64%)]\tLoss: 0.152616\t Accuracy:86.457%\n",
            "Epoch : 14 [44800/60000 (75%)]\tLoss: 0.227906\t Accuracy:86.554%\n",
            "Epoch : 14 [51200/60000 (85%)]\tLoss: 0.126492\t Accuracy:86.257%\n",
            "Epoch : 14 [57600/60000 (96%)]\tLoss: 0.259276\t Accuracy:86.322%\n",
            "Epoch : 15 [0/60000 (0%)]\tLoss: 0.076674\t Accuracy:93.750%\n",
            "Epoch : 15 [6400/60000 (11%)]\tLoss: 0.055730\t Accuracy:87.904%\n",
            "Epoch : 15 [12800/60000 (21%)]\tLoss: 0.380173\t Accuracy:87.484%\n",
            "Epoch : 15 [19200/60000 (32%)]\tLoss: 0.185402\t Accuracy:87.417%\n",
            "Epoch : 15 [25600/60000 (43%)]\tLoss: 0.136580\t Accuracy:87.621%\n",
            "Epoch : 15 [32000/60000 (53%)]\tLoss: 0.417486\t Accuracy:87.531%\n",
            "Epoch : 15 [38400/60000 (64%)]\tLoss: 0.130816\t Accuracy:87.651%\n",
            "Epoch : 15 [44800/60000 (75%)]\tLoss: 0.200582\t Accuracy:87.750%\n",
            "Epoch : 15 [51200/60000 (85%)]\tLoss: 0.098612\t Accuracy:87.434%\n",
            "Epoch : 15 [57600/60000 (96%)]\tLoss: 0.216147\t Accuracy:87.483%\n",
            "Epoch : 16 [0/60000 (0%)]\tLoss: 0.071052\t Accuracy:93.750%\n",
            "Epoch : 16 [6400/60000 (11%)]\tLoss: 0.049984\t Accuracy:88.899%\n",
            "Epoch : 16 [12800/60000 (21%)]\tLoss: 0.368358\t Accuracy:88.653%\n",
            "Epoch : 16 [19200/60000 (32%)]\tLoss: 0.134576\t Accuracy:88.602%\n",
            "Epoch : 16 [25600/60000 (43%)]\tLoss: 0.100392\t Accuracy:88.776%\n",
            "Epoch : 16 [32000/60000 (53%)]\tLoss: 0.331770\t Accuracy:88.689%\n",
            "Epoch : 16 [38400/60000 (64%)]\tLoss: 0.120250\t Accuracy:88.772%\n",
            "Epoch : 16 [44800/60000 (75%)]\tLoss: 0.189285\t Accuracy:88.762%\n",
            "Epoch : 16 [51200/60000 (85%)]\tLoss: 0.079888\t Accuracy:88.464%\n",
            "Epoch : 16 [57600/60000 (96%)]\tLoss: 0.167088\t Accuracy:88.486%\n",
            "Epoch : 17 [0/60000 (0%)]\tLoss: 0.071370\t Accuracy:93.750%\n",
            "Epoch : 17 [6400/60000 (11%)]\tLoss: 0.042180\t Accuracy:89.863%\n",
            "Epoch : 17 [12800/60000 (21%)]\tLoss: 0.359651\t Accuracy:89.666%\n",
            "Epoch : 17 [19200/60000 (32%)]\tLoss: 0.113883\t Accuracy:89.554%\n",
            "Epoch : 17 [25600/60000 (43%)]\tLoss: 0.090849\t Accuracy:89.626%\n",
            "Epoch : 17 [32000/60000 (53%)]\tLoss: 0.303167\t Accuracy:89.601%\n",
            "Epoch : 17 [38400/60000 (64%)]\tLoss: 0.100689\t Accuracy:89.701%\n",
            "Epoch : 17 [44800/60000 (75%)]\tLoss: 0.157760\t Accuracy:89.704%\n",
            "Epoch : 17 [51200/60000 (85%)]\tLoss: 0.077261\t Accuracy:89.358%\n",
            "Epoch : 17 [57600/60000 (96%)]\tLoss: 0.137031\t Accuracy:89.416%\n",
            "Epoch : 18 [0/60000 (0%)]\tLoss: 0.087377\t Accuracy:90.625%\n",
            "Epoch : 18 [6400/60000 (11%)]\tLoss: 0.043222\t Accuracy:90.392%\n",
            "Epoch : 18 [12800/60000 (21%)]\tLoss: 0.390077\t Accuracy:90.259%\n",
            "Epoch : 18 [19200/60000 (32%)]\tLoss: 0.100330\t Accuracy:90.178%\n",
            "Epoch : 18 [25600/60000 (43%)]\tLoss: 0.065770\t Accuracy:90.243%\n",
            "Epoch : 18 [32000/60000 (53%)]\tLoss: 0.240883\t Accuracy:90.247%\n",
            "Epoch : 18 [38400/60000 (64%)]\tLoss: 0.102773\t Accuracy:90.339%\n",
            "Epoch : 18 [44800/60000 (75%)]\tLoss: 0.139839\t Accuracy:90.346%\n",
            "Epoch : 18 [51200/60000 (85%)]\tLoss: 0.071394\t Accuracy:90.059%\n",
            "Epoch : 18 [57600/60000 (96%)]\tLoss: 0.115173\t Accuracy:90.143%\n",
            "Epoch : 19 [0/60000 (0%)]\tLoss: 0.080140\t Accuracy:93.750%\n",
            "Epoch : 19 [6400/60000 (11%)]\tLoss: 0.045315\t Accuracy:91.356%\n",
            "Epoch : 19 [12800/60000 (21%)]\tLoss: 0.381771\t Accuracy:91.069%\n",
            "Epoch : 19 [19200/60000 (32%)]\tLoss: 0.086762\t Accuracy:90.989%\n",
            "Epoch : 19 [25600/60000 (43%)]\tLoss: 0.058968\t Accuracy:90.953%\n",
            "Epoch : 19 [32000/60000 (53%)]\tLoss: 0.219613\t Accuracy:90.950%\n",
            "Epoch : 19 [38400/60000 (64%)]\tLoss: 0.097734\t Accuracy:91.023%\n",
            "Epoch : 19 [44800/60000 (75%)]\tLoss: 0.126821\t Accuracy:91.040%\n",
            "Epoch : 19 [51200/60000 (85%)]\tLoss: 0.086860\t Accuracy:90.781%\n",
            "Epoch : 19 [57600/60000 (96%)]\tLoss: 0.135272\t Accuracy:90.880%\n",
            "Epoch : 20 [0/60000 (0%)]\tLoss: 0.087881\t Accuracy:90.625%\n",
            "Epoch : 20 [6400/60000 (11%)]\tLoss: 0.044795\t Accuracy:91.791%\n",
            "Epoch : 20 [12800/60000 (21%)]\tLoss: 0.396330\t Accuracy:91.599%\n",
            "Epoch : 20 [19200/60000 (32%)]\tLoss: 0.083437\t Accuracy:91.623%\n",
            "Epoch : 20 [25600/60000 (43%)]\tLoss: 0.051955\t Accuracy:91.538%\n",
            "Epoch : 20 [32000/60000 (53%)]\tLoss: 0.166621\t Accuracy:91.571%\n",
            "Epoch : 20 [38400/60000 (64%)]\tLoss: 0.090570\t Accuracy:91.619%\n",
            "Epoch : 20 [44800/60000 (75%)]\tLoss: 0.124811\t Accuracy:91.618%\n",
            "Epoch : 20 [51200/60000 (85%)]\tLoss: 0.079710\t Accuracy:91.365%\n",
            "Epoch : 20 [57600/60000 (96%)]\tLoss: 0.117841\t Accuracy:91.465%\n",
            "Epoch : 21 [0/60000 (0%)]\tLoss: 0.078443\t Accuracy:93.750%\n",
            "Epoch : 21 [6400/60000 (11%)]\tLoss: 0.046243\t Accuracy:92.351%\n",
            "Epoch : 21 [12800/60000 (21%)]\tLoss: 0.396944\t Accuracy:92.184%\n",
            "Epoch : 21 [19200/60000 (32%)]\tLoss: 0.090678\t Accuracy:92.102%\n",
            "Epoch : 21 [25600/60000 (43%)]\tLoss: 0.047405\t Accuracy:91.971%\n",
            "Epoch : 21 [32000/60000 (53%)]\tLoss: 0.137052\t Accuracy:92.036%\n",
            "Epoch : 21 [38400/60000 (64%)]\tLoss: 0.067753\t Accuracy:92.108%\n",
            "Epoch : 21 [44800/60000 (75%)]\tLoss: 0.128936\t Accuracy:92.106%\n",
            "Epoch : 21 [51200/60000 (85%)]\tLoss: 0.076808\t Accuracy:91.878%\n",
            "Epoch : 21 [57600/60000 (96%)]\tLoss: 0.099776\t Accuracy:92.001%\n",
            "Epoch : 22 [0/60000 (0%)]\tLoss: 0.071799\t Accuracy:93.750%\n",
            "Epoch : 22 [6400/60000 (11%)]\tLoss: 0.043582\t Accuracy:92.662%\n",
            "Epoch : 22 [12800/60000 (21%)]\tLoss: 0.373491\t Accuracy:92.589%\n",
            "Epoch : 22 [19200/60000 (32%)]\tLoss: 0.078253\t Accuracy:92.559%\n",
            "Epoch : 22 [25600/60000 (43%)]\tLoss: 0.047531\t Accuracy:92.474%\n",
            "Epoch : 22 [32000/60000 (53%)]\tLoss: 0.141125\t Accuracy:92.595%\n",
            "Epoch : 22 [38400/60000 (64%)]\tLoss: 0.071583\t Accuracy:92.631%\n",
            "Epoch : 22 [44800/60000 (75%)]\tLoss: 0.124365\t Accuracy:92.586%\n",
            "Epoch : 22 [51200/60000 (85%)]\tLoss: 0.081421\t Accuracy:92.370%\n",
            "Epoch : 22 [57600/60000 (96%)]\tLoss: 0.113425\t Accuracy:92.495%\n",
            "Epoch : 23 [0/60000 (0%)]\tLoss: 0.080713\t Accuracy:93.750%\n",
            "Epoch : 23 [6400/60000 (11%)]\tLoss: 0.036822\t Accuracy:93.221%\n",
            "Epoch : 23 [12800/60000 (21%)]\tLoss: 0.357077\t Accuracy:93.111%\n",
            "Epoch : 23 [19200/60000 (32%)]\tLoss: 0.089312\t Accuracy:93.105%\n",
            "Epoch : 23 [25600/60000 (43%)]\tLoss: 0.043498\t Accuracy:92.958%\n",
            "Epoch : 23 [32000/60000 (53%)]\tLoss: 0.136370\t Accuracy:93.035%\n",
            "Epoch : 23 [38400/60000 (64%)]\tLoss: 0.066979\t Accuracy:93.089%\n",
            "Epoch : 23 [44800/60000 (75%)]\tLoss: 0.103090\t Accuracy:93.054%\n",
            "Epoch : 23 [51200/60000 (85%)]\tLoss: 0.080904\t Accuracy:92.837%\n",
            "Epoch : 23 [57600/60000 (96%)]\tLoss: 0.088681\t Accuracy:92.952%\n",
            "Epoch : 24 [0/60000 (0%)]\tLoss: 0.063960\t Accuracy:93.750%\n",
            "Epoch : 24 [6400/60000 (11%)]\tLoss: 0.036159\t Accuracy:93.579%\n",
            "Epoch : 24 [12800/60000 (21%)]\tLoss: 0.393515\t Accuracy:93.524%\n",
            "Epoch : 24 [19200/60000 (32%)]\tLoss: 0.117407\t Accuracy:93.547%\n",
            "Epoch : 24 [25600/60000 (43%)]\tLoss: 0.037308\t Accuracy:93.403%\n",
            "Epoch : 24 [32000/60000 (53%)]\tLoss: 0.131632\t Accuracy:93.450%\n",
            "Epoch : 24 [38400/60000 (64%)]\tLoss: 0.058344\t Accuracy:93.485%\n",
            "Epoch : 24 [44800/60000 (75%)]\tLoss: 0.099515\t Accuracy:93.447%\n",
            "Epoch : 24 [51200/60000 (85%)]\tLoss: 0.084660\t Accuracy:93.219%\n",
            "Epoch : 24 [57600/60000 (96%)]\tLoss: 0.077518\t Accuracy:93.321%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU2zLhyg92bu",
        "colab_type": "code",
        "outputId": "849deb1f-eca5-4789-dd4c-6fcbcfa6d112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test(net, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:9.800% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O_fyaSRRd60",
        "colab_type": "text"
      },
      "source": [
        "Let's try to hallucinate 7 from our trained network and see what happens. For this we will need to change the network slightly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh-KDmjyS-0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CT2FsVPEyYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_im(model, train_loader, digit = 7 ,iters = 10000, lossF = None):\n",
        "  im = torch.zeros_like(train_data[1][0]).view(1, 1, 28, 28).to(device)\n",
        "  im = Variable(im, requires_grad = True)\n",
        "  digit = Variable(torch.tensor(digit)).to(device).view(1)\n",
        "  if lossF == None:\n",
        "    lossF = nn.CrossEntropyLoss()\n",
        "  optim = torch.optim.Adam ([im, ], lr = 4e-1, weight_decay=1e-3)\n",
        "  model.train()\n",
        "  im_history = []\n",
        "  for it in range(iters):\n",
        "    \n",
        "    output = model(im)\n",
        "    # print(output)\n",
        "    loss = lossF (output, digit)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    \n",
        "    # if (it % 200) == 0:\n",
        "    #     print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
        "    #               epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
        "\n",
        "  return im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hcvC3TlR3Zs",
        "colab_type": "code",
        "outputId": "3c903cfd-1a54-4678-e389-333cbc1b2969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "im = train_im (net, train_loader, digit=7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-688bfb2b3ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_im\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-d04d69608ef0>\u001b[0m in \u001b[0;36mtrain_im\u001b[0;34m(model, train_loader, digit, iters, lossF)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossF\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/ATen/native/cuda/SoftMax.cu:651"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EdZELUFWWIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = np.asarray(im.view(28, 28).cpu().detach())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3o6tol1Zx1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neETLR1Ub0my",
        "colab_type": "text"
      },
      "source": [
        "In the cell below we will hallucinate all the digits from 0 to 9 each for 1000 iterations and will print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPoDpHbvZ9TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (10):\n",
        "  im = train_im (net, train_loader, digit=i)\n",
        "  im = np.asarray(im.view(28, 28).cpu().detach())\n",
        "  print (i)\n",
        "  plt.imshow(im)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKE-JtFKbXnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}